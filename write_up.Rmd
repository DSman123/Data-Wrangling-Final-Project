---
title: "Polarization: Using Reddit Data to Examine Trends in Thread Comment Behavior"
author: "Joel Cabrera, Neha Agarwal, & David Amiel"
subtitle: 'Data Wrangling & Husbandry: Spring 2022'
date: "4/8/2022"
output:
  html_document:
    toc: true
    toc_float: true
    theme: yeti
---
```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(RedditExtractoR)
library(tidytext)
library(textdata)
library(tidyverse)
library(ggpubr)
```

# Introduction
From behind the screen of our personal devices, we are able to share our thoughts with the world in seconds. Such ease and anonymity has created a culture where individuals feel more able to express negative, often hurtful opinions, and spead sentiments of racism^[1]^, xenophobia^[2]^, and homophobia^[3]^, among others. At the same time that access to social media is becoming more and more available, we see a stark increase in polarization in our society. The gap between sides of an issue is widening, and productive conversations among individuals with differing opinions are becoming less common. 

Of course, these two phenomena cannot be unrelated. Over a decade of research points to the complex relationship that exists between social media and polarization.^[4]^

In this project, we attempt to use data science techniques in R^[5]^ to explore, and potentially model, internet polarization. To do so, we have chosen to examine thread content, sentiment, and post popularity on Reddit, a popular networking forum site. We believe this choice in platform is both analytically and theoretically justified, as we are able to quickly glean large amounts of data from the site; its contents have already been shown to aid in predictive models of human behavior^[6]^, and the platform is home to some of the negative sentiments we hope to examine, like misogyny.^[7]^

# Project Overview
We began our project with an exploratory case study of what we had hoped to be a polarizing and charged thread on Reddit. The intention was to practice scraping the web data, performing analyses and creating visualizations, and build an initial predictive model regarding post popularity. However, our model was less successful than we had hoped, and we encountered a number of analytic shortcomings along the way.

Our initial Reddit thread was selected for the initial exploration because it contains the single most down-voted post on the history of the site. However, that was not indicative of the polarization we were looking for. In fact, the extremely unpopular opinion that originally attracted us to the thread was likely the reason we were unsuccessful: in being such an unpopular opinion, there is a consensus among users. Since everyone agreed to dislike the post, there wasn't much room for polarization.

We then rethought the strategy, instead turning towards threads based on their topic, rather than basing the decision off a metric like post popularity. We turned to two more polarizing themes: climate change and politics.^[8]^

# Case Study: EA's Blunder
Although this case study was ultimately relatively uninformative, we include it in this report as it illustrates our initial work of scraping web data, creating necessary ancillary and analytic variables, and the process and thinking behind the decision to turn our attention elsewhere. Further, it was the topic originally outlined in our project proposal, so we felt the need to give it some attention.

## Thread Background
In 2017, Electronic Arts (EA) released the sequel to a popular videogame - Battlefront II. Unlike its prequel, the game was relatively unpopular and generated a lot of backlash from the gaming community. One player took to Reddit to express their disdain, and EA issued a response. The specific complaint that they responded to dealt with a player mentioning that Darth Vader, a popular character in the franchise, was locked behind a paywall, even after paying for the game. EA's response suggested that their thinking behind this decision was to help players have a sense of "pride and accomplishment" for unlocking such characters. The thread text is below.^[9]^

**Seriously? I paid 80$ to have Vader locked?** This is a joke. I'll be contacting EA support for a refund... I can't even playing f***ing Darth Vader?!?!? Disgusting. This age of "micro-transactions" has gone WAY too far. Leave it to EA though to stretch the boundaries.

"The intent is to provide players with a sense of pride and accomplishment for unlocking different heroes.

As for cost, we selected initial values based upon data from the Open Beta and other adjustments made to milestone rewards before launch. Among other things, we're looking at average per-player credit earn rates on a daily basis, and we'll be making constant adjustments to ensure that players have challenges that are compelling, rewarding, and of course attainable via gameplay. 

We appreciate the candid feedback, and the passion the community has put forth around the current topics here on Reddit, our forums and across numerous social media outlets.

--- EA Community Team"

The original post has about 200,000 upvotes, while EA's response holds the record for downvotes, with a net score at nearly -700,000.

## Data Scraping
To assist in gathering data from Reddit, we found that any Reddit page can be 'converted' into the JSON format simply by adding a ".json" to the end of the URL. Using R, we imported the JSON file into a dataframe. However, the resulting structure was largely un-usable, as special characters within posts, usernames, or otherwise seemed to cause perturbation to the structure of the data. However, after some online research, we were able to locate RedditExtractoR^[10]^, a data extraction toolkit that allowed us to quickly pull in the data. Comments were stored as a sub-table within the extracted data, and we were quickly able to clean up any messy imports.

```{r caseStudy_dataScraping, echo=FALSE, include=FALSE}

# Import Comments from Target Thread
commentData <- tibble(get_thread_content("https://www.reddit.com/r/StarWarsBattlefront/comments/7cff0b/seriously_i_paid_80_to_have_vader_locked/dppum98/")$comments) %>% select(comment_id, comment, score)

```

## Modifying the Tibble
After gathering the comments in a tibble, we were able to use the tidytext^[11]^ package to pass each comment through the Bing, NRC, and AFINN lexicons for future sentiment analysis. To do so, three copies of the commentData tibble were created, and each was passed through one of the three lexicons. A copy of the one-word-per-row sentiment data from each lexicon was retained for future analysis. An inner join combined the original commentData with all sentiment data. Note that both the Bing and NRC lexicons have positive and negative sentiment data, and both values were retained. Based on comment data, words like "game" and "hero" were added to a custom stop word library (in this thread, a "hero" is a functional unit of the game, and does not carry the positive connotation assigned to it by lexicons).

After sentiment analysis information was completed, several additional variables were added to the dataset, including each post's parent popularity score (defined by the net up/down votes) and AFINN sentiment (which required some string manipulation of the comment_id using the stringr^[12]^ package), and a net sentiment score from both the Bing and NRC lexicons (defined as positive - negative). Below is a glimpse of the resulting tibble, which is used for all subsequent analyses.

```{r caseStudy_sentAnalysis&cleaning, echo=FALSE, include=FALSE}

# Custom Stopword Library
modStop <- bind_rows(stop_words, tibble(c("game", "gaming", "hero", "heroes", "ea", "pride", "accomplishment"))%>%mutate(lexicon="MANUAL"))

# Gather Sentiments for Each Comment from Bing, NRC, and AFINN 
nrcRaw    <- commentData %>% unnest_tokens(word, comment) %>% anti_join(bind_rows(modStop)) %>% inner_join(get_sentiments("nrc")) 
afinnRaw  <- commentData %>% unnest_tokens(word, comment) %>% anti_join(modStop) %>% inner_join(get_sentiments("afinn"))
bingRaw   <- commentData %>% unnest_tokens(word, comment) %>% anti_join(modStop) %>% inner_join(get_sentiments("bing")) 

afinnJoin <- afinnRaw  %>% group_by(comment_id)         %>% summarize(afinn=sum(value))
bingJoin  <- bingRaw   %>% count(comment_id, sentiment) %>% pivot_wider(values_from = "n", names_from = "sentiment")
nrcJoin   <- nrcRaw    %>% count(comment_id, sentiment) %>% pivot_wider(values_from = "n", names_from = "sentiment") %>% 
                           mutate(nrcPos=positive, nrcNeg=negative) %>% select(-`positive`, -`negative`)


# Combine Data into a Working Tibble
workingData <- inner_join(commentData, nrcJoin, by="comment_id") %>% inner_join(bingJoin, by="comment_id") %>% inner_join(afinnJoin, by="comment_id")

# Calculate Ancillary Variables
workingData <- workingData %>% mutate(nrcNetSent  = nrcPos - nrcNeg, 
                                      bingNetSent = positive - negative,
                                      parent_ID   = substr(comment_id, 1, regexpr("\\_[^\\_]*$", comment_id)-1))
workingData <- workingData %>% mutate(parentNRC   = workingData[match(parent_ID, workingData$comment_id), 16], 
                                      parentBing  = workingData[match(parent_ID, workingData$comment_id), 17],
                                      parentScore = workingData[match(parent_ID, workingData$comment_id), 3], 
                                      logScore    = log(abs(score))) %>% mutate_all(~replace(., is.na(.), 0))

```

```{r caseStudy_glimpse, include=TRUE, echo=FALSE}
glimpse(workingData)
```

## Exploring the Dataset {.tabset}
To get a better understanding for the comment section of the thread we are working with, we created numerous visualizations that highlight frequences, descriptive, and trends in the data with respect to word frequency, post popularity, and post sentiment.

### Descriptives: Sentiment
First, to see an overall picture of the sentiment of the thread, we created a simple histogram that maps overall post sentiment. Since word negative/positive binaries tended to 'cancel out' more frequently than the afinn database, the afinn data was used in the below histogram. We see that many posts are concentrated around neutral overall sentiment, but the thread is generally skewed negative. This makes sense, given that most of the thread are individuals criticizing EA's response.

```{r caseStudy_visualizations_sentiment1, echo=FALSE, include=TRUE, warnings=FALSE}

ggplot(data=workingData, mapping=aes(afinn)) + geom_histogram(binwidth = 1) + labs(x="Overall Post Sentiment (AFINN Subscore)", y="Frequency (n, number of posts)", title = "Distribution of Post Sentiment", subtitle = "Case Study: StarWars Battlefront Reddit Thread")

```
We were also interested in leveraging the specific sentiment data provided by the NRC lexicon to see if specific sentiments were more or less frequently observed than others. Bear in mind when interpreting the below (multiple plots were joined together using the ggpubr^[13]^ package) the relative frequency of positve and negative posts. 

```{r caseStudy_visualizations_sentiment2, echo=FALSE, include=TRUE, warnings=FALSE}
data<- workingData %>% mutate(binary = ifelse(afinn>0, "Positive", "Negative"))

anticipation <- ggplot(data=data, mapping=aes(anticipation, color=binary)) + geom_histogram(fill="white", alpha=0.5, position = "identity") + labs(y="Frequency", x="Anticipation Score (NRC)")

joy <- ggplot(data=data, mapping=aes(joy, color=binary)) + geom_histogram(fill="white", alpha=0.5, position = "identity") + labs(y="Frequency", x="Joy Score (NRC)")

surprise <- ggplot(data=data, mapping=aes(surprise, color=binary)) + geom_histogram(fill="white", alpha=0.5, position ="identity") + labs(y="Frequency", x="Surprise Score (NRC)")

trust <- ggplot(data=data, mapping=aes(trust, color=binary)) + geom_histogram(fill="white", alpha=0.5, position = "identity") + labs(y="Frequency", x="Trust Score (NRC)")

anger <- ggplot(data=data, mapping=aes(anger, color=binary)) + geom_histogram(fill="white", alpha=0.5, position = "identity") + labs(y="Frequency", x="Anger Score (NRC)")

disgust <- ggplot(data=data, mapping=aes(disgust, color=binary)) + geom_histogram(fill="white", alpha=0.5, position = "identity") + labs(y="Frequency", x="Disgust Score (NRC)")

fear <- ggplot(data=data, mapping=aes(fear, color=binary)) + geom_histogram(fill="white", alpha=0.5, position = "identity") + labs(y="Frequency", x="Fear Score (NRC)")

sadness <- ggplot(data=data, mapping=aes(sadness, color=binary)) + geom_histogram(fill="white", alpha=0.5, position = "identity") + labs(y="Frequency", x="Sadness Score (NRC)")


ggarrange(anticipation, joy, surprise, trust, anger, disgust, fear, sadness + rremove("y.text"), 
          ncol = 3, nrow = 3)


```

### Descriptives: Word Frequencies
reiuwfouncufiwean

### Trends: Popularity
ijijijiji

## Model Creation

## Case Study Takeaways

In our case studies, we found that the popularity of a parent post is a big indicator of a post’s potential to become popular. There was a high coefficient for parent popularity in our linear model which makes sense to us; commenting on an unpopular post doesn’t gain popularity; it is harder to ‘go viral’ from a lower-visibility starting point.

Additionally, unpopular posts lend themselves to more popular comment sections. There was a negative coefficient for parent sentiment in linear model which makes sense to us since there are more opportunities for a retaliation. This was interesting to see such a relationship with social media algorithms and negativity bias.

Lastly, the sentiment of a post seems to be less important than the sentiment of the post it responds to. There was a minute coefficient for post sentiment in linear model, so the linear model had little significance. More important seems to be the (mis)match in sentiment from parent and response.

# Case Study Limitations
There were some limitations that we encountered. For instance take the length of posts versus words contained in lexicons: roughly 10% of words are classified and most comments are already short. 
There were also a lack of posts with unpopular parents functionally, only EA’s post filled this role and also it was 
difficult to distinguish between negative parent and depth in the thread: 
“I wonder if Burger King wants to sell me a sense of pride and accomplishment by making me work 10 hours for my f**king fries."

# Broader Exploration: "The Weather and Everybody's Health" 


## In Brief: Other Analyzed Threads
We decided to look at Climate Change and Politics since those topics were the most polarized topics. The climate change model came from a reddit thread called “Are you concerned about climate change?" About 1.2k comments — using the same process, we see similar relationships (note that not all are statistically significant). In this case, parent popularity plays almost no role in determining how popular a post will be, but posts that respond to unpopular comments tend to get more likes. Similar to star wars, the sentiment of the post isn’t a big indicator in this model. 

The politics was extracted from a thread about politician Matt Dolan. In creating the same model, we see once again that parent popularity plays a role (similar to star wars thread), but parent sentiment does not contribute anything meaningful to this model. This might make sense, given that it takes a lot to ‘break through’ in discussions about politics, so you might need to comment on a popular thread in order to be seen, which might also have something to do about the size of the thread; this thread is very recent (still being updated) and is approaching 2.5k comments

## Assessing Model Applicability

# Synthesis: Key Takeaways
1) Serious variability in indicators of post popularity: This makes sense because social media is complex, however there seems to be some relationships with how ‘serious’ and polarized the topic is since we are only looking at a small piece of the puzzle (parent and child popularity and sentiment).

2)Limited capabilities of sentiment analysis: The process explored in class works well with much larger inputs. Our lexicons don’t account for context such as sarcasm, which carries the opposite sentiment than what is actually stated. Two factors really compounded this problem: internet comments are short & few words can be characterized.
	
3)Interesting qualitative data: This process can be applied and reapplied to other threads and there are many 
interesting visualizations to explore by thread and characteristic.

# References

[1] Matamoros-Fernández, A., & Farkas, J. (2021). Racism, Hate Speech, and Social Media: A Systematic Review and Critique. *Television & New Media, 22*(2), 205–224. https://doi.org/10.1177/1527476420982230

[2] Chenzi, V. (2021). Fake news, social media and xenophobia in South Africa. *African Identities, 19*(4), 502-521. https://doi.org/10.1080/14725843.2020.1804321 

[3] Ștefăniță, O., & Buf, D.-M.. (2021). Hate Speech in Social Media and Its Effects on the LGBT Community: A Review of the Current Research. *Romanian Journal of Communication and Public Relations, 23*(1), 47. https://doi.org/10.21018/rjcpr.2021.1.322

[4] Tucker, Guess, A., Barbera, P., Vaccari, C., Siegel, A., Sanovich, S., Stukal, D., & Nyhan, B. (2018). Social Media, Political Polarization, and Political Disinformation: A Review of the Scientific Literature. *SSRN Electronic Journal.* https://doi.org/10.2139/ssrn.3144139

[5] R Core Team (2022). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.

[6] Gjurković, M., & Šnajder, J. (2018). *Reddit: A Gold Mine for Personality Prediction.* https://doi.org/10.18653/v1/W18-1112 

[7] Farrell, T., Fernandez, M., Novotny, J., & Alani, H. (2019). *Exploring Misogyny across the Manosphere in Reddit* Proceedings of the 10th ACM Conference on Web Science, Boston, Massachusetts, USA. https://doi.org/10.1145/3292522.3326045

[8] Kubin, E., & von Sikorski, C. (2021). The role of (social) media in political polarization: a systematic review. *Annals of the International Communication Association, 45*(3), 188-206. https://doi.org/10.1080/23808985.2021.1976070 

[9] Reddit's Most Downvoted Comment: https://www.reddit.com/r/StarWarsBattlefront/comments/7cff0b/seriously_i_paid_80_to_have_vader_locked/dppum98/

[10] Rivera, Ivan (2022). RedditExtractoR: Reddit Data Extraction Toolkit. R package version 3.0.6. https://CRAN.R-project.org/package=RedditExtractoR

[11] Silge J, Robinson D (2016). “tidytext: Text Mining and Analysis Using Tidy Data Principles in R.” _JOSS_,
*1*(3). doi: 10.21105/joss.00037 (URL: https://doi.org/10.21105/joss.00037), <URL:
http://dx.doi.org/10.21105/joss.00037>.

[12] Wickham, Hadley (2019). stringr: Simple, Consistent Wrappers for Common String Operations. R package version 1.4.0.
  https://CRAN.R-project.org/package=stringr
  
[13] Kassambara, Alboukadel (2020). ggpubr: 'ggplot2' Based Publication Ready Plots. R package version 0.4.0.
  https://CRAN.R-project.org/package=ggpubr
